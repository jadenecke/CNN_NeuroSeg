import lasagne
import theano
import theano.tensor as T
import random
import math
import numpy as np
from lasagne.layers import batch_norm
from lasagne.init import Normal

def fc(patch_width, patch_var, pos_var, loc_var, target_var, num_conv_layers):

    # Network architecture
    input1 = lasagne.layers.InputLayer(shape=(None,1,patch_width,patch_width,patch_width), input_var=patch_var);
    input2 = lasagne.layers.InputLayer(shape=(None,1,patch_width,patch_width,patch_width), input_var=pos_var);
    c = lasagne.layers.ConcatLayer([input1, input2], axis=1)
    
    c = lasagne.layers.Conv3DLayer(c, num_filters=32, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
    for _ in range(1,num_conv_layers+1):
        c = lasagne.layers.Conv3DLayer(c, num_filters=32, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)

    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu) 
    c = lasagne.layers.Conv3DLayer(c, num_filters=1, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.sigmoid)
    network = lasagne.layers.FlattenLayer(c, outdim=2)
   
    return network;

def fc_loc(patch_width, patch_var, pos_var, loc_var, target_var, end_filters):

    # Network architecture
    input1 = lasagne.layers.InputLayer(shape=(None,1,patch_width,patch_width,patch_width), input_var=patch_var);
    input2 = lasagne.layers.InputLayer(shape=(None,1,patch_width,patch_width,patch_width), input_var=pos_var);
    c0 = lasagne.layers.ConcatLayer([input1, input2], axis=1)

    c1 = lasagne.layers.Conv3DLayer(c0, num_filters=end_filters/4, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
    c2 = lasagne.layers.Conv3DLayer(c1, num_filters=end_filters/4, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
    c3 = lasagne.layers.Conv3DLayer(c2, num_filters=end_filters/2, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
    c4 = lasagne.layers.Conv3DLayer(c3, num_filters=end_filters/2, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
    
    input3 = lasagne.layers.InputLayer(shape=(None,3,3,3,3), input_var=loc_var);
    input3 = lasagne.layers.ScaleLayer(input3,scales=lasagne.init.Constant(1e-2)); input3 = lasagne.layers.BiasLayer(input3,b=lasagne.init.Constant(0))
    c4 = lasagne.layers.ConcatLayer([c4,input3])

    c5 = lasagne.layers.Conv3DLayer(c4, num_filters=end_filters, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu) 
    
    c6 = lasagne.layers.Conv3DLayer(c5, num_filters=1, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.sigmoid)
    network = lasagne.layers.FlattenLayer(c6, outdim=2)    
   
    return network;

def fc_res(input_patch_radius, output_patch_radius, patch_var, pos_var, loc_var, target_var, location, prior, num_modalities, num_labels):

    input_patch_width = 2*input_patch_radius + 1
    output_patch_width = 2*output_patch_radius + 1
    num_conv_layers = input_patch_radius - output_patch_radius

    # Network architecture
    input1 = lasagne.layers.InputLayer(shape=(None,num_modalities,input_patch_width,input_patch_width,input_patch_width), input_var=patch_var)
    if (num_modalities == 1):
       input1 = lasagne.layers.ExpressionLayer(input1, lambda X: T.addbroadcast(X, 1), lambda s: s)

    if (prior == 1):
        input2 = lasagne.layers.InputLayer(shape=(None,1,input_patch_width,input_patch_width,input_patch_width), input_var=pos_var)
        c = lasagne.layers.ConcatLayer([input1, input2], axis=1)
    else:
        c = input1

    if (location == 1):
        input3 = lasagne.layers.InputLayer(shape=(None,3,input_patch_width,input_patch_width,input_patch_width), input_var=loc_var)
        c = lasagne.layers.ConcatLayer([c, input3], axis=1)

    c = lasagne.layers.BatchNormLayer(c)

    for layer in range(1,num_conv_layers+1):
        c = lasagne.layers.Conv3DLayer(c, num_filters=32, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
        if (layer == 1):
           c2 = c
        if (layer > 1):
           c2 = lasagne.layers.ConcatLayer([c2,c], axis=1, cropping=(None,None,'center','center','center'))

    c = lasagne.layers.BatchNormLayer(c2)

    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu)
    c = lasagne.layers.DropoutLayer(c, p=0.1)
    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu)
    c = lasagne.layers.DropoutLayer(c, p=0.1)
    c = lasagne.layers.Conv3DLayer(c, num_filters=num_labels, filter_size=(1,1,1), nonlinearity=None)
  
    # Hacky spatial softmax
    c = lasagne.layers.DimshuffleLayer(c,(1,0,2,3,4))
    c = lasagne.layers.FlattenLayer(c, outdim=2)
    c = lasagne.layers.DimshuffleLayer(c,(1,0))
    network = lasagne.layers.NonlinearityLayer(c, nonlinearity=lasagne.nonlinearities.softmax)

    shaped = lasagne.layers.DimshuffleLayer(network,(1,0))

    target_shape = list(c.input_layer.input_layer.output_shape);
    target_shape[1] = -1
    target_shape = tuple(target_shape)

    shaped = lasagne.layers.ReshapeLayer(shaped, target_shape)
    shaped = lasagne.layers.DimshuffleLayer(shaped,(1,0,2,3,4))

    return (network, shaped);


def fc_cascade(input_patch_radius, output_patch_radius, patch_var, pos_var, loc_var, target_var, location, prior, num_modalities, num_labels):

    input_patch_width = 2*input_patch_radius + 1
    output_patch_width = 2*output_patch_radius + 1
    num_conv_layers = (input_patch_radius - output_patch_radius)/2

    # Network architecture 1
    input1 = lasagne.layers.InputLayer(shape=(None,num_modalities,input_patch_width,input_patch_width,input_patch_width), input_var=patch_var)

    if (num_modalities == 1):
        input1 = lasagne.layers.ExpressionLayer(input1, lambda X: T.addbroadcast(X, 1), lambda s: s)
    if (prior == 1):
        input2 = lasagne.layers.InputLayer(shape=(None,1,input_patch_width,input_patch_width,input_patch_width), input_var=pos_var)
        c = lasagne.layers.ConcatLayer([input1, input2], axis=1)
    else:
        c = input1

    if (location == 1):
        input3 = lasagne.layers.InputLayer(shape=(None,3,input_patch_width,input_patch_width,input_patch_width), input_var=loc_var)
        c = lasagne.layers.ConcatLayer([c, input3], axis=1)

    c = lasagne.layers.BatchNormLayer(c)

    for layer in range(1,num_conv_layers+1):
        c = lasagne.layers.Conv3DLayer(c, num_filters=32, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
        if (layer == 1):
           c2 = c
        if (layer > 1):
           c2 = lasagne.layers.ConcatLayer([c2,c], axis=1, cropping=(None,None,'center','center','center'))

    c = lasagne.layers.BatchNormLayer(c2)

    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu)
    c = lasagne.layers.DropoutLayer(c, p=0.1)
    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu)
    c = lasagne.layers.DropoutLayer(c, p=0.1)
    c = lasagne.layers.Conv3DLayer(c, num_filters=num_labels, filter_size=(1,1,1), nonlinearity=None)
  
    # Hacky spatial softmax
    c = lasagne.layers.DimshuffleLayer(c,(1,0,2,3,4))
    c = lasagne.layers.FlattenLayer(c, outdim=2)
    c = lasagne.layers.DimshuffleLayer(c,(1,0))
    network1 = lasagne.layers.NonlinearityLayer(c, nonlinearity=lasagne.nonlinearities.softmax)

    shaped = lasagne.layers.DimshuffleLayer(network,(1,0))

    target_shape = list(c.input_layer.input_layer.output_shape);
    target_shape[1] = -1
    target_shape = tuple(target_shape)

    shaped = lasagne.layers.ReshapeLayer(shaped, target_shape)
    shaped = lasagne.layers.DimshuffleLayer(shaped,(1,0,2,3,4))

    # "shaped' has size B.N.X.Y.Z where B is batch size and N is number of labels

    # Network architecture 2
    c = lasagne.layers.ConcatLayer([shaped, input1, input3], axis=1, cropping=(None,None,'center','center','center'))

    c = lasagne.layers.BatchNormLayer(c)

    for layer in range(1,num_conv_layers+1):
        c = lasagne.layers.Conv3DLayer(c, num_filters=32, filter_size=(3,3,3), nonlinearity=lasagne.nonlinearities.elu)
        if (layer == 1):
           c2 = c
        if (layer > 1):
           c2 = lasagne.layers.ConcatLayer([c2,c], axis=1, cropping=(None,None,'center','center','center'))
    
    c = lasagne.layers.BatchNormLayer(c2)

    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu)
    c = lasagne.layers.DropoutLayer(c, p=0.1)
    c = lasagne.layers.Conv3DLayer(c, num_filters=64, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.elu)
    c = lasagne.layers.DropoutLayer(c, p=0.1)
    c = lasagne.layers.Conv3DLayer(c, num_filters=1, filter_size=(1,1,1), nonlinearity=lasagne.nonlinearities.sigmoid)

    network2 = lasagne.layers.FlattenLayer(c, outdim=2)

    # Hacky spatial softmax
    c = lasagne.layers.DimshuffleLayer(c,(1,0,2,3,4))
    c = lasagne.layers.FlattenLayer(c, outdim=2)
    c = lasagne.layers.DimshuffleLayer(c,(1,0))
    network1 = lasagne.layers.NonlinearityLayer(c, nonlinearity=lasagne.nonlinearities.softmax)

    shaped = lasagne.layers.DimshuffleLayer(network,(1,0))

    target_shape = list(c.input_layer.input_layer.output_shape);
    target_shape[1] = -1
    target_shape = tuple(target_shape)

    shaped = lasagne.layers.ReshapeLayer(shaped, target_shape)
    shaped = lasagne.layers.DimshuffleLayer(shaped,(1,0,2,3,4))

    return (network1, network2, shaped)

class HalveLayer(lasagne.layers.Layer):
    def get_output_for(self, input, **kwargs):
        return 0.5 * input

